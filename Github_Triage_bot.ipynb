{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghat0tkach/github-triage-bot/blob/main/Github_Triage_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cosine\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rZ4id4UpSR5n"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Ghat0tkach/jlug-lenscape-event-frontend.git\n",
        "!pip install transformers torch tqdm\n",
        "!pip install groq\n",
        "!pip install faiss-cpu sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt_kTQaSRjNB"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FbIJQrXpSbZQ"
      },
      "outputs": [],
      "source": [
        "def chunk_code(code, max_length=510):\n",
        "    tokens = tokenizer.tokenize(code)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_length):\n",
        "        chunk = tokens[i:i + max_length]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embeddings(code_chunk):\n",
        "    inputs = tokenizer(code_chunk, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r', errors='ignore') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    chunks = chunk_code(content)\n",
        "    embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = get_embeddings(chunk).astype(np.float32)\n",
        "        embeddings.append(embedding)\n",
        "        metadata.append({\n",
        "            'file_path': file_path,\n",
        "            'chunk_index': i,\n",
        "            'chunk_content': chunk\n",
        "        })\n",
        "\n",
        "    return embeddings, metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V23ch2DSrR2",
        "outputId": "7c69d0a2-c75a-460d-d08b-776312c1e78a"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CgwkEbQZSdjg",
        "outputId": "6796ebaf-9d21-4d67-c673-2b7d393eb7b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_repository(repo_path):\n",
        "    all_embeddings = []\n",
        "    all_metadata = []\n",
        "\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in tqdm(files, desc=\"Processing files\"):\n",
        "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.html', '.css','.jsx','.tsx','.ts')):  # Add more extensions as needed\n",
        "                print(\"File \", file)\n",
        "                file_path = os.path.join(root, file)\n",
        "                embeddings, metadata = process_file(file_path)\n",
        "                all_embeddings.extend(embeddings)\n",
        "                all_metadata.extend(metadata)\n",
        "\n",
        "    return all_embeddings, all_metadata\n",
        "\n",
        "# Process the repository\n",
        "repo_path = 'jlug-lenscape-event-frontend'  # Adjust this to the cloned repo's path\n",
        "embeddings, metadata = process_repository(repo_path)\n",
        "\n",
        "print(f\"Total embeddings generated: {len(embeddings)}\")\n",
        "print(f\"Sample embedding shape: {embeddings[0].shape}\")\n",
        "print(f\"Sample metadata: {metadata[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INCCBkhWVT58",
        "outputId": "a3889b2e-792a-46d2-99e2-821b69224954"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save embeddings\n",
        "embeddings_array = np.array([e[0] for e in embeddings])\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "\n",
        "# Save metadata\n",
        "with open('metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "print(\"Embeddings saved to 'embeddings.npy'\")\n",
        "print(\"Metadata saved to 'metadata.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "993ufnTeWdcN"
      },
      "outputs": [],
      "source": [
        "# Load embeddings\n",
        "loaded_embeddings = np.load('embeddings.npy')\n",
        "\n",
        "# Load metadata\n",
        "with open('metadata.json', 'r') as f:\n",
        "    loaded_metadata = json.load(f)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(loaded_embeddings)\n",
        "\n",
        "# Add metadata columns\n",
        "df['file_path'] = [item['file_path'] for item in loaded_metadata]\n",
        "df['chunk_index'] = [item['chunk_index'] for item in loaded_metadata]\n",
        "\n",
        "# Rename embedding columns\n",
        "df.columns = [f'dim_{i}' if isinstance(i, int) else i for i in df.columns]\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Display info about the DataFrame\n",
        "print(df.info())\n",
        "\n",
        "# If you want to see all columns, you can use:\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# print(df)\n",
        "\n",
        "# Save as CSV if needed\n",
        "df.to_csv('embeddings_with_metadata.csv', index=False)\n",
        "print(\"Saved embeddings with metadata to 'embeddings_with_metadata.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAISS index setup and similarity search function\n",
        "def setup_faiss_index(embeddings):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "index = setup_faiss_index(embeddings)\n",
        "\n",
        "# Function to search similar code snippets using FAISS\n",
        "def find_similar_code_faiss(query_text, index, embeddings, metadata, top_k=3):\n",
        "    query_embedding = get_embeddings(query_text).astype(np.float32)\n",
        "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
        "    relevant_snippets = [metadata[i] for i in indices[0]]\n",
        "    return relevant_snippets, distances[0]\n",
        "\n",
        "# Sample query to test similarity search\n",
        "query_text = \"How to implement binary search\"\n",
        "relevant_snippets, distances = find_similar_code_faiss(query_text, index, embeddings, metadata)\n",
        "for snippet, distance in zip(relevant_snippets, distances):\n",
        "    print(f\"File: {snippet['file_path']}, Distance: {distance}, Content: {snippet['chunk_content']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "tiRCgz3pXeJ0",
        "outputId": "6a30c3df-7b59-4e26-d247-04d08d98f5ad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from scipy.spatial.distance import cosine\n",
        "from groq import Groq\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "def encode_query(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
        "\n",
        "# Load embeddings and metadata\n",
        "embeddings = np.load('embeddings.npy')\n",
        "with open('metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "def find_relevant_snippets(query, embeddings, metadata, top_k=3):\n",
        "    query_embedding = encode_query(query)\n",
        "    similarities = [1 - cosine(query_embedding, emb) for emb in embeddings]\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return [metadata[i] for i in top_indices], [similarities[i] for i in top_indices]\n",
        "\n",
        "# Function to query Groq API\n",
        "client = Groq(api_key=\"\")\n",
        "\n",
        "def query_groq(question, context, similarities):\n",
        "    prompt = f\"\"\"You are an AI assistant specialized in answering questions about code.\n",
        "    Given the following code snippets, their relevance scores, and a question, provide a detailed answer.\n",
        "    Use the relevance scores to weight the importance of each snippet in your answer.\n",
        "\n",
        "    Code snippets and their relevance scores:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Main question-answering loop\n",
        "while True:\n",
        "    question = input(\"Ask a question about the code (or type 'exit' to quit): \")\n",
        "    if question.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    relevant_snippets, similarities = find_relevant_snippets(question, embeddings, metadata)\n",
        "    context = \"\\n\\n\".join([f\"File: {snippet['file_path']}\\nRelevance: {sim:.4f}\\nChunk: {snippet['chunk_content']}\"\n",
        "                           for snippet, sim in zip(relevant_snippets, similarities)])\n",
        "\n",
        "    answer = query_groq(question, context, similarities)\n",
        "    print(\"\\nAnswer:\", answer)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNApGFxx7LIVqarvTFkIELq",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
