{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghat0tkach/github-triage-bot/blob/main/Github_Triage_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rZ4id4UpSR5n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "!git clone https://github.com/Ghat0tkach/jlug-lenscape-event-frontend.git\n",
        "!pip install transformers torch tqdm\n",
        "!pip install groq\n",
        "!pip install faiss-cpu sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt_kTQaSRjNB"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FbIJQrXpSbZQ"
      },
      "outputs": [],
      "source": [
        "def chunk_code(code, max_length=510):\n",
        "    tokens = tokenizer.tokenize(code)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_length):\n",
        "        chunk = tokens[i:i + max_length]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embeddings(code_chunk):\n",
        "    inputs = tokenizer(code_chunk, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    chunks = chunk_code(content)\n",
        "    embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = get_embeddings(chunk)\n",
        "        embeddings.append(embedding)\n",
        "        metadata.append({\n",
        "            'file_path': file_path,\n",
        "            'chunk_index': i,\n",
        "            'chunk_content': chunk\n",
        "        })\n",
        "\n",
        "    return embeddings, metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V23ch2DSrR2",
        "outputId": "7c69d0a2-c75a-460d-d08b-776312c1e78a"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CgwkEbQZSdjg",
        "outputId": "6796ebaf-9d21-4d67-c673-2b7d393eb7b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_repository(repo_path):\n",
        "    all_embeddings = []\n",
        "    all_metadata = []\n",
        "\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in tqdm(files, desc=\"Processing files\"):\n",
        "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.html', '.css','.jsx','.tsx','.ts')):  # Add more extensions as needed\n",
        "                print(\"File \", file)\n",
        "                file_path = os.path.join(root, file)\n",
        "                embeddings, metadata = process_file(file_path)\n",
        "                all_embeddings.extend(embeddings)\n",
        "                all_metadata.extend(metadata)\n",
        "\n",
        "    return all_embeddings, all_metadata\n",
        "\n",
        "# Process the repository\n",
        "repo_path = 'jlug-lenscape-event-frontend'  # Adjust this to the cloned repo's path\n",
        "embeddings, metadata = process_repository(repo_path)\n",
        "\n",
        "print(f\"Total embeddings generated: {len(embeddings)}\")\n",
        "print(f\"Sample embedding shape: {embeddings[0].shape}\")\n",
        "print(f\"Sample metadata: {metadata[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INCCBkhWVT58",
        "outputId": "a3889b2e-792a-46d2-99e2-821b69224954"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_array = np.array([e[0] for e in embeddings])\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "\n",
        "# Save metadata\n",
        "with open('metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "print(\"Embeddings saved to 'embeddings.npy'\")\n",
        "print(\"Metadata saved to 'metadata.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "993ufnTeWdcN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load embeddings\n",
        "loaded_embeddings = np.load('embeddings.npy')\n",
        "\n",
        "# Load metadata\n",
        "with open('metadata.json', 'r') as f:\n",
        "    loaded_metadata = json.load(f)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(loaded_embeddings)\n",
        "\n",
        "# Add metadata columns\n",
        "df['file_path'] = [item['file_path'] for item in loaded_metadata]\n",
        "df['chunk_index'] = [item['chunk_index'] for item in loaded_metadata]\n",
        "\n",
        "# Rename embedding columns\n",
        "df.columns = [f'dim_{i}' if isinstance(i, int) else i for i in df.columns]\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Display info about the DataFrame\n",
        "print(df.info())\n",
        "\n",
        "# If you want to see all columns, you can use:\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# print(df)\n",
        "\n",
        "# Save as CSV if needed\n",
        "df.to_csv('embeddings_with_metadata.csv', index=False)\n",
        "print(\"Saved embeddings with metadata to 'embeddings_with_metadata.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "def get_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# Example: Convert existing embeddings for FAISS indexing\n",
        "def setup_faiss_index(embeddings):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Load embeddings and metadata, then build the FAISS index\n",
        "embeddings = np.load('embeddings.npy')\n",
        "index = setup_faiss_index(embeddings)\n",
        "\n",
        "def find_similar_code_faiss(query_text, index, embeddings, metadata, top_k=3):\n",
        "    # Get query embedding\n",
        "    query_embedding = get_embeddings(query_text).astype(np.float32)\n",
        "    \n",
        "    # Search FAISS for nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    \n",
        "    # Retrieve the top relevant code snippets\n",
        "    relevant_snippets = [metadata[i] for i in indices[0]]\n",
        "    return relevant_snippets, distances[0]\n",
        "\n",
        "# Example usage\n",
        "query_text = \"How to implement binary search\"\n",
        "relevant_snippets, distances = find_similar_code_faiss(query_text, index, embeddings, metadata)\n",
        "for snippet, distance in zip(relevant_snippets, distances):\n",
        "    print(f\"File: {snippet['file_path']}, Distance: {distance}, Content: {snippet['chunk_content']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "tiRCgz3pXeJ0",
        "outputId": "6a30c3df-7b59-4e26-d247-04d08d98f5ad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from scipy.spatial.distance import cosine\n",
        "from groq import Groq\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "def encode_query(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
        "\n",
        "# Load embeddings and metadata\n",
        "embeddings = np.load('embeddings.npy')\n",
        "with open('metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "def find_relevant_snippets(query, embeddings, metadata, top_k=3):\n",
        "    query_embedding = encode_query(query)\n",
        "    similarities = [1 - cosine(query_embedding, emb) for emb in embeddings]\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return [metadata[i] for i in top_indices], [similarities[i] for i in top_indices]\n",
        "\n",
        "# Function to query Groq API\n",
        "client = Groq(api_key=\"\")\n",
        "\n",
        "def query_groq(question, context, similarities):\n",
        "    prompt = f\"\"\"You are an AI assistant specialized in answering questions about code.\n",
        "    Given the following code snippets, their relevance scores, and a question, provide a detailed answer.\n",
        "    Use the relevance scores to weight the importance of each snippet in your answer.\n",
        "\n",
        "    Code snippets and their relevance scores:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Main question-answering loop\n",
        "while True:\n",
        "    question = input(\"Ask a question about the code (or type 'exit' to quit): \")\n",
        "    if question.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    relevant_snippets, similarities = find_relevant_snippets(question, embeddings, metadata)\n",
        "    context = \"\\n\\n\".join([f\"File: {snippet['file_path']}\\nRelevance: {sim:.4f}\\nChunk: {snippet['chunk_content']}\"\n",
        "                           for snippet, sim in zip(relevant_snippets, similarities)])\n",
        "\n",
        "    answer = query_groq(question, context, similarities)\n",
        "    print(\"\\nAnswer:\", answer)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNApGFxx7LIVqarvTFkIELq",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
